{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***UIT_CS115*** - \n",
    "***FINAL PROJECT***\n",
    "\n",
    "_**TOPIC** : POLICY GRADIENT FOR REINFORCEMENT LEARNING_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor-Critic algorithm is a type of **Reinforcement Learning** algorithm that combines aspects of both Policy-based methods (Actor) and Value-based methods (Critic). This hybrid approach is designed to address the limitations of each method when used individually.\n",
    "\n",
    "In the actor-critic framework, an agent (the \"actor\") learns a policy to make decisions, and a value function (the \"Critic\") evaluates the actions taken by the Actor.\n",
    "\n",
    "Simultaneously, the critic evaluates these actions by estimating their value or quality. This dual role allows the method to strike a balance between exploration and exploitation, leveraging the strengths of both policy and value functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.reset()\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + 0.99 * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tReward: 13.00\tAverage reward: 13.00\n",
      "Episode 2\tReward: 10.00\tAverage reward: 11.50\n",
      "Episode 3\tReward: 10.00\tAverage reward: 11.00\n",
      "Episode 4\tReward: 10.00\tAverage reward: 10.75\n",
      "Episode 5\tReward: 10.00\tAverage reward: 10.60\n",
      "Episode 6\tReward: 8.00\tAverage reward: 10.17\n",
      "Episode 7\tReward: 10.00\tAverage reward: 10.14\n",
      "Episode 8\tReward: 9.00\tAverage reward: 10.00\n",
      "Episode 9\tReward: 9.00\tAverage reward: 9.89\n",
      "Episode 10\tReward: 9.00\tAverage reward: 9.80\n",
      "Episode 11\tReward: 9.00\tAverage reward: 9.73\n",
      "Episode 12\tReward: 10.00\tAverage reward: 9.75\n",
      "Episode 13\tReward: 10.00\tAverage reward: 9.77\n",
      "Episode 14\tReward: 8.00\tAverage reward: 9.64\n",
      "Episode 15\tReward: 11.00\tAverage reward: 9.73\n",
      "Episode 16\tReward: 9.00\tAverage reward: 9.69\n",
      "Episode 17\tReward: 10.00\tAverage reward: 9.71\n",
      "Episode 18\tReward: 12.00\tAverage reward: 9.83\n",
      "Episode 19\tReward: 9.00\tAverage reward: 9.79\n",
      "Episode 20\tReward: 10.00\tAverage reward: 9.80\n",
      "Episode 21\tReward: 9.00\tAverage reward: 9.76\n",
      "Episode 22\tReward: 10.00\tAverage reward: 9.77\n",
      "Episode 23\tReward: 9.00\tAverage reward: 9.74\n",
      "Episode 24\tReward: 8.00\tAverage reward: 9.67\n",
      "Episode 25\tReward: 10.00\tAverage reward: 9.68\n",
      "Episode 26\tReward: 10.00\tAverage reward: 9.69\n",
      "Episode 27\tReward: 9.00\tAverage reward: 9.67\n",
      "Episode 28\tReward: 9.00\tAverage reward: 9.64\n",
      "Episode 29\tReward: 9.00\tAverage reward: 9.62\n",
      "Episode 30\tReward: 10.00\tAverage reward: 9.63\n",
      "Episode 31\tReward: 10.00\tAverage reward: 9.65\n",
      "Episode 32\tReward: 10.00\tAverage reward: 9.66\n",
      "Episode 33\tReward: 10.00\tAverage reward: 9.67\n",
      "Episode 34\tReward: 10.00\tAverage reward: 9.68\n",
      "Episode 35\tReward: 9.00\tAverage reward: 9.66\n",
      "Episode 36\tReward: 10.00\tAverage reward: 9.67\n",
      "Episode 37\tReward: 10.00\tAverage reward: 9.68\n",
      "Episode 38\tReward: 10.00\tAverage reward: 9.68\n",
      "Episode 39\tReward: 9.00\tAverage reward: 9.67\n",
      "Episode 40\tReward: 8.00\tAverage reward: 9.62\n",
      "Episode 41\tReward: 10.00\tAverage reward: 9.63\n",
      "Episode 42\tReward: 8.00\tAverage reward: 9.60\n",
      "Episode 43\tReward: 10.00\tAverage reward: 9.60\n",
      "Episode 44\tReward: 10.00\tAverage reward: 9.61\n",
      "Episode 45\tReward: 8.00\tAverage reward: 9.58\n",
      "Episode 46\tReward: 10.00\tAverage reward: 9.59\n",
      "Episode 47\tReward: 9.00\tAverage reward: 9.57\n",
      "Episode 48\tReward: 8.00\tAverage reward: 9.54\n",
      "Episode 49\tReward: 9.00\tAverage reward: 9.53\n",
      "Episode 50\tReward: 9.00\tAverage reward: 9.52\n",
      "Solved! Running reward is now 476.0 and the last episode runs to 9 time steps!\n",
      "Episode 51\tReward: 11.00\tAverage reward: 9.55\n",
      "Solved! Running reward is now 487.0 and the last episode runs to 11 time steps!\n",
      "Episode 52\tReward: 9.00\tAverage reward: 9.54\n",
      "Solved! Running reward is now 496.0 and the last episode runs to 9 time steps!\n",
      "Episode 53\tReward: 9.00\tAverage reward: 9.53\n",
      "Solved! Running reward is now 505.0 and the last episode runs to 9 time steps!\n",
      "Episode 54\tReward: 9.00\tAverage reward: 9.52\n",
      "Solved! Running reward is now 514.0 and the last episode runs to 9 time steps!\n",
      "Episode 55\tReward: 9.00\tAverage reward: 9.51\n",
      "Solved! Running reward is now 523.0 and the last episode runs to 9 time steps!\n",
      "Episode 56\tReward: 9.00\tAverage reward: 9.50\n",
      "Solved! Running reward is now 532.0 and the last episode runs to 9 time steps!\n",
      "Episode 57\tReward: 9.00\tAverage reward: 9.49\n",
      "Solved! Running reward is now 541.0 and the last episode runs to 9 time steps!\n",
      "Episode 58\tReward: 9.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 550.0 and the last episode runs to 9 time steps!\n",
      "Episode 59\tReward: 9.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 559.0 and the last episode runs to 9 time steps!\n",
      "Episode 60\tReward: 9.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 568.0 and the last episode runs to 9 time steps!\n",
      "Episode 61\tReward: 8.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 576.0 and the last episode runs to 8 time steps!\n",
      "Episode 62\tReward: 8.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 584.0 and the last episode runs to 8 time steps!\n",
      "Episode 63\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 594.0 and the last episode runs to 10 time steps!\n",
      "Episode 64\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 604.0 and the last episode runs to 10 time steps!\n",
      "Episode 65\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 614.0 and the last episode runs to 10 time steps!\n",
      "Episode 66\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 624.0 and the last episode runs to 10 time steps!\n",
      "Episode 67\tReward: 9.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 633.0 and the last episode runs to 9 time steps!\n",
      "Episode 68\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 642.0 and the last episode runs to 9 time steps!\n",
      "Episode 69\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 652.0 and the last episode runs to 10 time steps!\n",
      "Episode 70\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 661.0 and the last episode runs to 9 time steps!\n",
      "Episode 71\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 670.0 and the last episode runs to 9 time steps!\n",
      "Episode 72\tReward: 9.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 679.0 and the last episode runs to 9 time steps!\n",
      "Episode 73\tReward: 8.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 687.0 and the last episode runs to 8 time steps!\n",
      "Episode 74\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 697.0 and the last episode runs to 10 time steps!\n",
      "Episode 75\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 706.0 and the last episode runs to 9 time steps!\n",
      "Episode 76\tReward: 8.00\tAverage reward: 9.39\n",
      "Solved! Running reward is now 714.0 and the last episode runs to 8 time steps!\n",
      "Episode 77\tReward: 10.00\tAverage reward: 9.40\n",
      "Solved! Running reward is now 724.0 and the last episode runs to 10 time steps!\n",
      "Episode 78\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 734.0 and the last episode runs to 10 time steps!\n",
      "Episode 79\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 744.0 and the last episode runs to 10 time steps!\n",
      "Episode 80\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 753.0 and the last episode runs to 9 time steps!\n",
      "Episode 81\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 763.0 and the last episode runs to 10 time steps!\n",
      "Episode 82\tReward: 8.00\tAverage reward: 9.40\n",
      "Solved! Running reward is now 771.0 and the last episode runs to 8 time steps!\n",
      "Episode 83\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 781.0 and the last episode runs to 10 time steps!\n",
      "Episode 84\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 791.0 and the last episode runs to 10 time steps!\n",
      "Episode 85\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 801.0 and the last episode runs to 10 time steps!\n",
      "Episode 86\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 810.0 and the last episode runs to 9 time steps!\n",
      "Episode 87\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 820.0 and the last episode runs to 10 time steps!\n",
      "Episode 88\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 830.0 and the last episode runs to 10 time steps!\n",
      "Episode 89\tReward: 9.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 839.0 and the last episode runs to 9 time steps!\n",
      "Episode 90\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 848.0 and the last episode runs to 9 time steps!\n",
      "Episode 91\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 858.0 and the last episode runs to 10 time steps!\n",
      "Episode 92\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 868.0 and the last episode runs to 10 time steps!\n",
      "Episode 93\tReward: 9.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 877.0 and the last episode runs to 9 time steps!\n",
      "Episode 94\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 887.0 and the last episode runs to 10 time steps!\n",
      "Episode 95\tReward: 9.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 896.0 and the last episode runs to 9 time steps!\n",
      "Episode 96\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 906.0 and the last episode runs to 10 time steps!\n",
      "Episode 97\tReward: 8.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 914.0 and the last episode runs to 8 time steps!\n",
      "Episode 98\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 924.0 and the last episode runs to 10 time steps!\n",
      "Episode 99\tReward: 8.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 932.0 and the last episode runs to 8 time steps!\n",
      "Episode 100\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 941.0 and the last episode runs to 9 time steps!\n",
      "Episode 101\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 951.0 and the last episode runs to 10 time steps!\n",
      "Episode 102\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 960.0 and the last episode runs to 9 time steps!\n",
      "Episode 103\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 970.0 and the last episode runs to 10 time steps!\n",
      "Episode 104\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 979.0 and the last episode runs to 9 time steps!\n",
      "Episode 105\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 988.0 and the last episode runs to 9 time steps!\n",
      "Episode 106\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 997.0 and the last episode runs to 9 time steps!\n",
      "Episode 107\tReward: 9.00\tAverage reward: 9.40\n",
      "Solved! Running reward is now 1006.0 and the last episode runs to 9 time steps!\n",
      "Episode 108\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1016.0 and the last episode runs to 10 time steps!\n",
      "Episode 109\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1026.0 and the last episode runs to 10 time steps!\n",
      "Episode 110\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1036.0 and the last episode runs to 10 time steps!\n",
      "Episode 111\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1046.0 and the last episode runs to 10 time steps!\n",
      "Episode 112\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1056.0 and the last episode runs to 10 time steps!\n",
      "Episode 113\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1065.0 and the last episode runs to 9 time steps!\n",
      "Episode 114\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1074.0 and the last episode runs to 9 time steps!\n",
      "Episode 115\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1084.0 and the last episode runs to 10 time steps!\n",
      "Episode 116\tReward: 8.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1092.0 and the last episode runs to 8 time steps!\n",
      "Episode 117\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1102.0 and the last episode runs to 10 time steps!\n",
      "Episode 118\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1111.0 and the last episode runs to 9 time steps!\n",
      "Episode 119\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1120.0 and the last episode runs to 9 time steps!\n",
      "Episode 120\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1129.0 and the last episode runs to 9 time steps!\n",
      "Episode 121\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1139.0 and the last episode runs to 10 time steps!\n",
      "Episode 122\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1148.0 and the last episode runs to 9 time steps!\n",
      "Episode 123\tReward: 10.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1158.0 and the last episode runs to 10 time steps!\n",
      "Episode 124\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1168.0 and the last episode runs to 10 time steps!\n",
      "Episode 125\tReward: 9.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1177.0 and the last episode runs to 9 time steps!\n",
      "Episode 126\tReward: 9.00\tAverage reward: 9.41\n",
      "Solved! Running reward is now 1186.0 and the last episode runs to 9 time steps!\n",
      "Episode 127\tReward: 10.00\tAverage reward: 9.42\n",
      "Solved! Running reward is now 1196.0 and the last episode runs to 10 time steps!\n",
      "Episode 128\tReward: 11.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1207.0 and the last episode runs to 11 time steps!\n",
      "Episode 129\tReward: 10.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1217.0 and the last episode runs to 10 time steps!\n",
      "Episode 130\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1227.0 and the last episode runs to 10 time steps!\n",
      "Episode 131\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1237.0 and the last episode runs to 10 time steps!\n",
      "Episode 132\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1246.0 and the last episode runs to 9 time steps!\n",
      "Episode 133\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1256.0 and the last episode runs to 10 time steps!\n",
      "Episode 134\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1265.0 and the last episode runs to 9 time steps!\n",
      "Episode 135\tReward: 10.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1275.0 and the last episode runs to 10 time steps!\n",
      "Episode 136\tReward: 8.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1283.0 and the last episode runs to 8 time steps!\n",
      "Episode 137\tReward: 9.00\tAverage reward: 9.43\n",
      "Solved! Running reward is now 1292.0 and the last episode runs to 9 time steps!\n",
      "Episode 138\tReward: 11.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1303.0 and the last episode runs to 11 time steps!\n",
      "Episode 139\tReward: 11.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 1314.0 and the last episode runs to 11 time steps!\n",
      "Episode 140\tReward: 8.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1322.0 and the last episode runs to 8 time steps!\n",
      "Episode 141\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 1332.0 and the last episode runs to 10 time steps!\n",
      "Episode 142\tReward: 9.00\tAverage reward: 9.44\n",
      "Solved! Running reward is now 1341.0 and the last episode runs to 9 time steps!\n",
      "Episode 143\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 1351.0 and the last episode runs to 10 time steps!\n",
      "Episode 144\tReward: 10.00\tAverage reward: 9.45\n",
      "Solved! Running reward is now 1361.0 and the last episode runs to 10 time steps!\n",
      "Episode 145\tReward: 10.00\tAverage reward: 9.46\n",
      "Solved! Running reward is now 1371.0 and the last episode runs to 10 time steps!\n",
      "Episode 146\tReward: 12.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 1383.0 and the last episode runs to 12 time steps!\n",
      "Episode 147\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1393.0 and the last episode runs to 10 time steps!\n",
      "Episode 148\tReward: 9.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 1402.0 and the last episode runs to 9 time steps!\n",
      "Episode 149\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1412.0 and the last episode runs to 10 time steps!\n",
      "Episode 150\tReward: 8.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 1420.0 and the last episode runs to 8 time steps!\n",
      "Episode 151\tReward: 10.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 1430.0 and the last episode runs to 10 time steps!\n",
      "Episode 152\tReward: 10.00\tAverage reward: 9.47\n",
      "Solved! Running reward is now 1440.0 and the last episode runs to 10 time steps!\n",
      "Episode 153\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1450.0 and the last episode runs to 10 time steps!\n",
      "Episode 154\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1460.0 and the last episode runs to 10 time steps!\n",
      "Episode 155\tReward: 9.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1469.0 and the last episode runs to 9 time steps!\n",
      "Episode 156\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1479.0 and the last episode runs to 10 time steps!\n",
      "Episode 157\tReward: 10.00\tAverage reward: 9.48\n",
      "Solved! Running reward is now 1489.0 and the last episode runs to 10 time steps!\n",
      "Episode 158\tReward: 15.00\tAverage reward: 9.52\n",
      "Solved! Running reward is now 1504.0 and the last episode runs to 15 time steps!\n",
      "Episode 159\tReward: 14.00\tAverage reward: 9.55\n",
      "Solved! Running reward is now 1518.0 and the last episode runs to 14 time steps!\n",
      "Episode 160\tReward: 10.00\tAverage reward: 9.55\n",
      "Solved! Running reward is now 1528.0 and the last episode runs to 10 time steps!\n",
      "Episode 161\tReward: 17.00\tAverage reward: 9.60\n",
      "Solved! Running reward is now 1545.0 and the last episode runs to 17 time steps!\n",
      "Episode 162\tReward: 15.00\tAverage reward: 9.63\n",
      "Solved! Running reward is now 1560.0 and the last episode runs to 15 time steps!\n",
      "Episode 163\tReward: 41.00\tAverage reward: 9.82\n",
      "Solved! Running reward is now 1601.0 and the last episode runs to 41 time steps!\n",
      "Episode 164\tReward: 24.00\tAverage reward: 9.91\n",
      "Solved! Running reward is now 1625.0 and the last episode runs to 24 time steps!\n",
      "Episode 165\tReward: 36.00\tAverage reward: 10.07\n",
      "Solved! Running reward is now 1661.0 and the last episode runs to 36 time steps!\n",
      "Episode 166\tReward: 51.00\tAverage reward: 10.31\n",
      "Solved! Running reward is now 1712.0 and the last episode runs to 51 time steps!\n",
      "Episode 167\tReward: 48.00\tAverage reward: 10.54\n",
      "Solved! Running reward is now 1760.0 and the last episode runs to 48 time steps!\n",
      "Episode 168\tReward: 74.00\tAverage reward: 10.92\n",
      "Solved! Running reward is now 1834.0 and the last episode runs to 74 time steps!\n",
      "Episode 169\tReward: 61.00\tAverage reward: 11.21\n",
      "Solved! Running reward is now 1895.0 and the last episode runs to 61 time steps!\n",
      "Episode 170\tReward: 113.00\tAverage reward: 11.81\n",
      "Solved! Running reward is now 2008.0 and the last episode runs to 113 time steps!\n",
      "Episode 171\tReward: 42.00\tAverage reward: 11.99\n",
      "Solved! Running reward is now 2050.0 and the last episode runs to 42 time steps!\n",
      "Episode 172\tReward: 19.00\tAverage reward: 12.03\n",
      "Solved! Running reward is now 2069.0 and the last episode runs to 19 time steps!\n",
      "Episode 173\tReward: 33.00\tAverage reward: 12.15\n",
      "Solved! Running reward is now 2102.0 and the last episode runs to 33 time steps!\n",
      "Episode 174\tReward: 20.00\tAverage reward: 12.20\n",
      "Solved! Running reward is now 2122.0 and the last episode runs to 20 time steps!\n",
      "Episode 175\tReward: 38.00\tAverage reward: 12.34\n",
      "Solved! Running reward is now 2160.0 and the last episode runs to 38 time steps!\n",
      "Episode 176\tReward: 39.00\tAverage reward: 12.49\n",
      "Solved! Running reward is now 2199.0 and the last episode runs to 39 time steps!\n",
      "Episode 177\tReward: 51.00\tAverage reward: 12.71\n",
      "Solved! Running reward is now 2250.0 and the last episode runs to 51 time steps!\n",
      "Episode 178\tReward: 43.00\tAverage reward: 12.88\n",
      "Solved! Running reward is now 2293.0 and the last episode runs to 43 time steps!\n",
      "Episode 179\tReward: 90.00\tAverage reward: 13.31\n",
      "Solved! Running reward is now 2383.0 and the last episode runs to 90 time steps!\n",
      "Episode 180\tReward: 73.00\tAverage reward: 13.64\n",
      "Solved! Running reward is now 2456.0 and the last episode runs to 73 time steps!\n",
      "Episode 181\tReward: 72.00\tAverage reward: 13.97\n",
      "Solved! Running reward is now 2528.0 and the last episode runs to 72 time steps!\n",
      "Episode 182\tReward: 93.00\tAverage reward: 14.40\n",
      "Solved! Running reward is now 2621.0 and the last episode runs to 93 time steps!\n",
      "Episode 183\tReward: 77.00\tAverage reward: 14.74\n",
      "Solved! Running reward is now 2698.0 and the last episode runs to 77 time steps!\n",
      "Episode 184\tReward: 109.00\tAverage reward: 15.26\n",
      "Solved! Running reward is now 2807.0 and the last episode runs to 109 time steps!\n",
      "Episode 185\tReward: 66.00\tAverage reward: 15.53\n",
      "Solved! Running reward is now 2873.0 and the last episode runs to 66 time steps!\n",
      "Episode 186\tReward: 73.00\tAverage reward: 15.84\n",
      "Solved! Running reward is now 2946.0 and the last episode runs to 73 time steps!\n",
      "Episode 187\tReward: 60.00\tAverage reward: 16.07\n",
      "Solved! Running reward is now 3006.0 and the last episode runs to 60 time steps!\n",
      "Episode 188\tReward: 76.00\tAverage reward: 16.39\n",
      "Solved! Running reward is now 3082.0 and the last episode runs to 76 time steps!\n",
      "Episode 189\tReward: 20.00\tAverage reward: 16.41\n",
      "Solved! Running reward is now 3102.0 and the last episode runs to 20 time steps!\n",
      "Episode 190\tReward: 45.00\tAverage reward: 16.56\n",
      "Solved! Running reward is now 3147.0 and the last episode runs to 45 time steps!\n",
      "Episode 191\tReward: 9.00\tAverage reward: 16.52\n",
      "Solved! Running reward is now 3156.0 and the last episode runs to 9 time steps!\n",
      "Episode 192\tReward: 11.00\tAverage reward: 16.49\n",
      "Solved! Running reward is now 3167.0 and the last episode runs to 11 time steps!\n",
      "Episode 193\tReward: 41.00\tAverage reward: 16.62\n",
      "Solved! Running reward is now 3208.0 and the last episode runs to 41 time steps!\n",
      "Episode 194\tReward: 47.00\tAverage reward: 16.78\n",
      "Solved! Running reward is now 3255.0 and the last episode runs to 47 time steps!\n",
      "Episode 195\tReward: 19.00\tAverage reward: 16.79\n",
      "Solved! Running reward is now 3274.0 and the last episode runs to 19 time steps!\n",
      "Episode 196\tReward: 62.00\tAverage reward: 17.02\n",
      "Solved! Running reward is now 3336.0 and the last episode runs to 62 time steps!\n",
      "Episode 197\tReward: 98.00\tAverage reward: 17.43\n",
      "Solved! Running reward is now 3434.0 and the last episode runs to 98 time steps!\n",
      "Episode 198\tReward: 98.00\tAverage reward: 17.84\n",
      "Solved! Running reward is now 3532.0 and the last episode runs to 98 time steps!\n",
      "Episode 199\tReward: 122.00\tAverage reward: 18.36\n",
      "Solved! Running reward is now 3654.0 and the last episode runs to 122 time steps!\n",
      "Episode 200\tReward: 140.00\tAverage reward: 18.97\n",
      "Solved! Running reward is now 3794.0 and the last episode runs to 140 time steps!\n",
      "Episode 201\tReward: 116.00\tAverage reward: 19.45\n",
      "Solved! Running reward is now 3910.0 and the last episode runs to 116 time steps!\n",
      "Episode 202\tReward: 160.00\tAverage reward: 20.15\n",
      "Solved! Running reward is now 4070.0 and the last episode runs to 160 time steps!\n",
      "Episode 203\tReward: 214.00\tAverage reward: 21.10\n",
      "Solved! Running reward is now 4284.0 and the last episode runs to 214 time steps!\n",
      "Episode 204\tReward: 130.00\tAverage reward: 21.64\n",
      "Solved! Running reward is now 4414.0 and the last episode runs to 130 time steps!\n",
      "Episode 205\tReward: 138.00\tAverage reward: 22.20\n",
      "Solved! Running reward is now 4552.0 and the last episode runs to 138 time steps!\n",
      "Episode 206\tReward: 117.00\tAverage reward: 22.67\n",
      "Solved! Running reward is now 4669.0 and the last episode runs to 117 time steps!\n",
      "Episode 207\tReward: 113.00\tAverage reward: 23.10\n",
      "Solved! Running reward is now 4782.0 and the last episode runs to 113 time steps!\n",
      "Episode 208\tReward: 106.00\tAverage reward: 23.50\n",
      "Solved! Running reward is now 4888.0 and the last episode runs to 106 time steps!\n",
      "Episode 209\tReward: 21.00\tAverage reward: 23.49\n",
      "Solved! Running reward is now 4909.0 and the last episode runs to 21 time steps!\n",
      "Episode 210\tReward: 95.00\tAverage reward: 23.83\n",
      "Solved! Running reward is now 5004.0 and the last episode runs to 95 time steps!\n",
      "Episode 211\tReward: 115.00\tAverage reward: 24.26\n",
      "Solved! Running reward is now 5119.0 and the last episode runs to 115 time steps!\n",
      "Episode 212\tReward: 121.00\tAverage reward: 24.72\n",
      "Solved! Running reward is now 5240.0 and the last episode runs to 121 time steps!\n",
      "Episode 213\tReward: 144.00\tAverage reward: 25.28\n",
      "Solved! Running reward is now 5384.0 and the last episode runs to 144 time steps!\n",
      "Episode 214\tReward: 173.00\tAverage reward: 25.97\n",
      "Solved! Running reward is now 5557.0 and the last episode runs to 173 time steps!\n",
      "Episode 215\tReward: 209.00\tAverage reward: 26.82\n",
      "Solved! Running reward is now 5766.0 and the last episode runs to 209 time steps!\n",
      "Episode 216\tReward: 221.00\tAverage reward: 27.72\n",
      "Solved! Running reward is now 5987.0 and the last episode runs to 221 time steps!\n",
      "Episode 217\tReward: 223.00\tAverage reward: 28.62\n",
      "Solved! Running reward is now 6210.0 and the last episode runs to 223 time steps!\n",
      "Episode 218\tReward: 387.00\tAverage reward: 30.26\n",
      "Solved! Running reward is now 6597.0 and the last episode runs to 387 time steps!\n",
      "Episode 219\tReward: 393.00\tAverage reward: 31.92\n",
      "Solved! Running reward is now 6990.0 and the last episode runs to 393 time steps!\n",
      "Episode 220\tReward: 468.00\tAverage reward: 33.90\n",
      "Solved! Running reward is now 7458.0 and the last episode runs to 468 time steps!\n",
      "Episode 221\tReward: 872.00\tAverage reward: 37.69\n",
      "Solved! Running reward is now 8330.0 and the last episode runs to 872 time steps!\n",
      "Episode 222\tReward: 635.00\tAverage reward: 40.38\n",
      "Solved! Running reward is now 8965.0 and the last episode runs to 635 time steps!\n",
      "Episode 223\tReward: 3750.00\tAverage reward: 57.02\n",
      "Solved! Running reward is now 12715.0 and the last episode runs to 3750 time steps!\n",
      "Episode 224\tReward: 1357.00\tAverage reward: 62.82\n",
      "Solved! Running reward is now 14072.0 and the last episode runs to 1357 time steps!\n",
      "Episode 225\tReward: 967.00\tAverage reward: 66.84\n",
      "Solved! Running reward is now 15039.0 and the last episode runs to 967 time steps!\n",
      "Episode 226\tReward: 657.00\tAverage reward: 69.45\n",
      "Solved! Running reward is now 15696.0 and the last episode runs to 657 time steps!\n",
      "Episode 227\tReward: 614.00\tAverage reward: 71.85\n",
      "Solved! Running reward is now 16310.0 and the last episode runs to 614 time steps!\n",
      "Episode 228\tReward: 46.00\tAverage reward: 71.74\n",
      "Solved! Running reward is now 16356.0 and the last episode runs to 46 time steps!\n",
      "Episode 229\tReward: 814.00\tAverage reward: 74.98\n",
      "Solved! Running reward is now 17170.0 and the last episode runs to 814 time steps!\n",
      "Episode 230\tReward: 43.00\tAverage reward: 74.84\n",
      "Solved! Running reward is now 17213.0 and the last episode runs to 43 time steps!\n",
      "Episode 231\tReward: 148.00\tAverage reward: 75.16\n",
      "Solved! Running reward is now 17361.0 and the last episode runs to 148 time steps!\n",
      "Episode 232\tReward: 54.00\tAverage reward: 75.06\n",
      "Solved! Running reward is now 17415.0 and the last episode runs to 54 time steps!\n",
      "Episode 233\tReward: 62.00\tAverage reward: 75.01\n",
      "Solved! Running reward is now 17477.0 and the last episode runs to 62 time steps!\n",
      "Episode 234\tReward: 147.00\tAverage reward: 75.32\n",
      "Solved! Running reward is now 17624.0 and the last episode runs to 147 time steps!\n",
      "Episode 235\tReward: 156.00\tAverage reward: 75.66\n",
      "Solved! Running reward is now 17780.0 and the last episode runs to 156 time steps!\n",
      "Episode 236\tReward: 29.00\tAverage reward: 75.46\n",
      "Solved! Running reward is now 17809.0 and the last episode runs to 29 time steps!\n",
      "Episode 237\tReward: 64.00\tAverage reward: 75.41\n",
      "Solved! Running reward is now 17873.0 and the last episode runs to 64 time steps!\n",
      "Episode 238\tReward: 127.00\tAverage reward: 75.63\n",
      "Solved! Running reward is now 18000.0 and the last episode runs to 127 time steps!\n",
      "Episode 239\tReward: 34.00\tAverage reward: 75.46\n",
      "Solved! Running reward is now 18034.0 and the last episode runs to 34 time steps!\n",
      "Episode 240\tReward: 80.00\tAverage reward: 75.47\n",
      "Solved! Running reward is now 18114.0 and the last episode runs to 80 time steps!\n",
      "Episode 241\tReward: 522.00\tAverage reward: 77.33\n",
      "Solved! Running reward is now 18636.0 and the last episode runs to 522 time steps!\n",
      "Episode 242\tReward: 240.00\tAverage reward: 78.00\n",
      "Solved! Running reward is now 18876.0 and the last episode runs to 240 time steps!\n",
      "Episode 243\tReward: 250.00\tAverage reward: 78.71\n",
      "Solved! Running reward is now 19126.0 and the last episode runs to 250 time steps!\n",
      "Episode 244\tReward: 303.00\tAverage reward: 79.63\n",
      "Solved! Running reward is now 19429.0 and the last episode runs to 303 time steps!\n",
      "Episode 245\tReward: 140.00\tAverage reward: 79.87\n",
      "Solved! Running reward is now 19569.0 and the last episode runs to 140 time steps!\n",
      "Episode 246\tReward: 160.00\tAverage reward: 80.20\n",
      "Solved! Running reward is now 19729.0 and the last episode runs to 160 time steps!\n",
      "Episode 247\tReward: 136.00\tAverage reward: 80.43\n",
      "Solved! Running reward is now 19865.0 and the last episode runs to 136 time steps!\n",
      "Episode 248\tReward: 138.00\tAverage reward: 80.66\n",
      "Solved! Running reward is now 20003.0 and the last episode runs to 138 time steps!\n",
      "Episode 249\tReward: 150.00\tAverage reward: 80.94\n",
      "Solved! Running reward is now 20153.0 and the last episode runs to 150 time steps!\n",
      "Episode 250\tReward: 256.00\tAverage reward: 81.64\n",
      "Solved! Running reward is now 20409.0 and the last episode runs to 256 time steps!\n",
      "Episode 251\tReward: 140.00\tAverage reward: 81.87\n",
      "Solved! Running reward is now 20549.0 and the last episode runs to 140 time steps!\n",
      "Episode 252\tReward: 127.00\tAverage reward: 82.05\n",
      "Solved! Running reward is now 20676.0 and the last episode runs to 127 time steps!\n",
      "Episode 253\tReward: 127.00\tAverage reward: 82.23\n",
      "Solved! Running reward is now 20803.0 and the last episode runs to 127 time steps!\n",
      "Episode 254\tReward: 161.00\tAverage reward: 82.54\n",
      "Solved! Running reward is now 20964.0 and the last episode runs to 161 time steps!\n",
      "Episode 255\tReward: 130.00\tAverage reward: 82.72\n",
      "Solved! Running reward is now 21094.0 and the last episode runs to 130 time steps!\n",
      "Episode 256\tReward: 115.00\tAverage reward: 82.85\n",
      "Solved! Running reward is now 21209.0 and the last episode runs to 115 time steps!\n",
      "Episode 257\tReward: 24.00\tAverage reward: 82.62\n",
      "Solved! Running reward is now 21233.0 and the last episode runs to 24 time steps!\n",
      "Episode 258\tReward: 114.00\tAverage reward: 82.74\n",
      "Solved! Running reward is now 21347.0 and the last episode runs to 114 time steps!\n",
      "Episode 259\tReward: 131.00\tAverage reward: 82.93\n",
      "Solved! Running reward is now 21478.0 and the last episode runs to 131 time steps!\n",
      "Episode 260\tReward: 145.00\tAverage reward: 83.17\n",
      "Solved! Running reward is now 21623.0 and the last episode runs to 145 time steps!\n",
      "Episode 261\tReward: 124.00\tAverage reward: 83.32\n",
      "Solved! Running reward is now 21747.0 and the last episode runs to 124 time steps!\n",
      "Episode 262\tReward: 126.00\tAverage reward: 83.48\n",
      "Solved! Running reward is now 21873.0 and the last episode runs to 126 time steps!\n",
      "Episode 263\tReward: 142.00\tAverage reward: 83.71\n",
      "Solved! Running reward is now 22015.0 and the last episode runs to 142 time steps!\n",
      "Episode 264\tReward: 137.00\tAverage reward: 83.91\n",
      "Solved! Running reward is now 22152.0 and the last episode runs to 137 time steps!\n",
      "Episode 265\tReward: 140.00\tAverage reward: 84.12\n",
      "Solved! Running reward is now 22292.0 and the last episode runs to 140 time steps!\n",
      "Episode 266\tReward: 123.00\tAverage reward: 84.27\n",
      "Solved! Running reward is now 22415.0 and the last episode runs to 123 time steps!\n",
      "Episode 267\tReward: 209.00\tAverage reward: 84.73\n",
      "Solved! Running reward is now 22624.0 and the last episode runs to 209 time steps!\n",
      "Episode 268\tReward: 215.00\tAverage reward: 85.22\n",
      "Solved! Running reward is now 22839.0 and the last episode runs to 215 time steps!\n",
      "Episode 269\tReward: 174.00\tAverage reward: 85.55\n",
      "Solved! Running reward is now 23013.0 and the last episode runs to 174 time steps!\n",
      "Episode 270\tReward: 169.00\tAverage reward: 85.86\n",
      "Solved! Running reward is now 23182.0 and the last episode runs to 169 time steps!\n",
      "Episode 271\tReward: 139.00\tAverage reward: 86.06\n",
      "Solved! Running reward is now 23321.0 and the last episode runs to 139 time steps!\n",
      "Episode 272\tReward: 195.00\tAverage reward: 86.46\n",
      "Solved! Running reward is now 23516.0 and the last episode runs to 195 time steps!\n",
      "Episode 273\tReward: 157.00\tAverage reward: 86.71\n",
      "Solved! Running reward is now 23673.0 and the last episode runs to 157 time steps!\n",
      "Episode 274\tReward: 267.00\tAverage reward: 87.37\n",
      "Solved! Running reward is now 23940.0 and the last episode runs to 267 time steps!\n",
      "Episode 275\tReward: 276.00\tAverage reward: 88.06\n",
      "Solved! Running reward is now 24216.0 and the last episode runs to 276 time steps!\n",
      "Episode 276\tReward: 313.00\tAverage reward: 88.87\n",
      "Solved! Running reward is now 24529.0 and the last episode runs to 313 time steps!\n",
      "Episode 277\tReward: 365.00\tAverage reward: 89.87\n",
      "Solved! Running reward is now 24894.0 and the last episode runs to 365 time steps!\n",
      "Episode 278\tReward: 588.00\tAverage reward: 91.66\n",
      "Solved! Running reward is now 25482.0 and the last episode runs to 588 time steps!\n",
      "Episode 279\tReward: 401.00\tAverage reward: 92.77\n",
      "Solved! Running reward is now 25883.0 and the last episode runs to 401 time steps!\n",
      "Episode 280\tReward: 458.00\tAverage reward: 94.08\n",
      "Solved! Running reward is now 26341.0 and the last episode runs to 458 time steps!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolved! Running reward is now \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe last episode runs to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m time steps!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(running_reward, t))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# take the action\u001b[39;00m\n\u001b[1;32m     19\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 20\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     23\u001b[0m ep_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/envs/classic_control/cartpole.py:297\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    running_reward = 0\n",
    "\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in count(1):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 100001):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "\n",
    "            # take the action\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = ep_reward +  running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        log_interval = 1\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tReward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward / i_episode))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
